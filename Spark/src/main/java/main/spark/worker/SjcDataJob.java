package main.spark.worker;

import jakarta.annotation.PostConstruct;
import lombok.extern.slf4j.Slf4j;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.springframework.stereotype.Component;

import static org.apache.spark.sql.functions.col;
import static org.apache.spark.sql.functions.regexp_replace;

@Slf4j
@Component
public class SjcDataJob {

    @PostConstruct
    public void sync() {
        log.info("üöÄ Starting SjcDataJob to read JSON from HDFS and save to DB...");

        // 1Ô∏è‚É£ Kh·ªüi t·∫°o SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("SjcDataToDatabase")
                .master("local[*]")
                .config("spark.hadoop.fs.defaultFS", "hdfs://192.168.38.88:9000")
                .getOrCreate();

        String hdfsPath = "hdfs://192.168.38.88:9000/user/totuong/data/sjc_prices.json";

        // 2Ô∏è‚É£ ƒê·ªçc JSON (multiline v√¨ file l√† m·∫£ng JSON)
        Dataset<Row> df = spark.read()
                .option("multiline", "true")
                .json(hdfsPath);

        log.info("‚úÖ Schema ban ƒë·∫ßu:");
        df.printSchema();

        Dataset<Row> cleaned = df
                .withColumn("buy_value",
                        regexp_replace(col("buy"), ",", "").cast("double"))
                .withColumn("sell_value",
                        regexp_replace(col("sell"), ",", "").cast("double"))
                .withColumn("branch_name", col("payload.BranchName"))
                .withColumn("type_name", col("payload.TypeName"))
                .select("date", "type", "buy_value", "sell_value", "branch_name", "type_name");

        log.info("‚úÖ D·ªØ li·ªáu sau khi l√†m s·∫°ch:");
        cleaned.show(10, false);

        // 4Ô∏è‚É£ C·∫•u h√¨nh k·∫øt n·ªëi JDBC
        String jdbcUrl = "jdbc:postgresql://192.168.38.10:5432/bigdata";

        log.info("‚úÖ ƒê√£ ghi d·ªØ li·ªáu v√†o b·∫£ng 'sjc_prices' th√†nh c√¥ng!");

        spark.stop();
    }

}
